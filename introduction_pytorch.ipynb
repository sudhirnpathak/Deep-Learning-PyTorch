{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Tutorial "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Contents**\n",
    "<br> 1. [Tensors](#tensors)\n",
    "<br> 2. [Introduction to Autograd](#autograd)\n",
    "<br> 3. [A simple illustration of Backpropagation using Autograd](#backpropagation)\n",
    "<br> 4. [Illustrative linear regression: A manual implementation](#linear_regression_manual)\n",
    "<br> 5. [Illustrative linear regression: A Pytorch implementation](#linear_regression_pytorch)\n",
    "<br> 6. [Linear Regression](#linear_regression)\n",
    "<br> 7. [Logistic Regression](#logistic_regression)\n",
    "<br> 8. [Batch Training: Dataset and DataLoader Classes](#dataset_dataloader)\n",
    "<br> 9. [Dataset Transforms](#dataset_transforms)\n",
    "<br> 10. [Softmax and Cross Entropy](#softmax_cross_entropy)\n",
    "<br> 11. [Activation Functions](#activation_functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='tensors'></a>\n",
    "## 1. Tensors\n",
    "\n",
    "Similar to NumPy where everything is based on arrays and vectors, everything is based on tensor operations in PyTorch. \n",
    "<br>Tensors can have different dimensions, 1d, 2d, 3d or higher..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating Tensors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(1)          # creating a simple tensor\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.empty(1)           # empty tensor with value not initialized\n",
    "print(x)\n",
    "\n",
    "x = torch.empty(3)           # one dimensional empty tensor, with three elements\n",
    "print(x)\n",
    "\n",
    "x = torch.empty(2, 3)        # two dimensional tensor\n",
    "print(x)\n",
    "\n",
    "x = torch.empty(2, 3, 4)     # three dimensional tensor\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.zeros(2, 2)         # two dimensional tensor with all zeros\n",
    "print(x)\n",
    "print('----------')\n",
    "\n",
    "x = torch.ones(2, 2)          # two dimensional tensor with all ones \n",
    "print(x)\n",
    "print('----------')\n",
    "\n",
    "x = torch.rand(2, 2)          # two dimensional tensor with random values\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(2, 2)            # size \n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Datatype**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(2, 2)                           # default datatype is float32\n",
    "print(x.dtype)\n",
    "\n",
    "x = torch.ones(2, 2, dtype = torch.int)        # we can define the data type\n",
    "print(x.dtype)\n",
    "\n",
    "x = torch.ones(2, 2, dtype = torch.double)     # double: float64\n",
    "print(x.dtype)\n",
    "\n",
    "x = torch.ones(2, 2, dtype = torch.float16)    # float16\n",
    "print(x.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(1)                 # default dtype is int64\n",
    "y = torch.tensor(1.0)               # default dytpe is float32\n",
    "print(x, y)\n",
    "print(x.dtype, y.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Basic Operations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(2, 2)            # element wise addition\n",
    "y = torch.rand(2, 2)\n",
    "z = x + y\n",
    "\n",
    "print(x)\n",
    "print(y) \n",
    "print(z)\n",
    "\n",
    "z = torch.add(x, y)             # another way for element wise addition\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(2, 2)\n",
    "y = torch.rand(2, 2)\n",
    "\n",
    "print(x)\n",
    "print(y)\n",
    "print('----------')\n",
    "\n",
    "z = torch.sub(x, y)            # element-wise subtraction\n",
    "print(z)\n",
    "\n",
    "z = torch.mul(x, y)            # element-wise multiplication\n",
    "print(z)\n",
    "\n",
    "z = torch.div(x, y)            # element-wise division\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(2, 2)\n",
    "y = torch.zeros(2, 2)\n",
    "\n",
    "print(y)\n",
    "y.add_(x)       # in place addition: y will be updated to the elementwise sum of x and y\n",
    "print(y)        # in pytorch, every function with trailing underscore does an inplace operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Slicing and Resizing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(2, 3)        # slicing\n",
    "\n",
    "print(x)\n",
    "print('-----------')\n",
    "\n",
    "print(x[:, 0])      # all the rows but column 0 (first column): returns in row format, not in column format\n",
    "print(x[1, :])      # second row and all the columns\n",
    "\n",
    "print(x[1, 1])         # returns the element at the indices as a tensor\n",
    "print(x[1, 1].item())  # we can use item() method to get the actual value, instead of the tensor\n",
    "                       # can be used only when we have one element in the tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(4, 4)         # reshaping \n",
    "print(x)\n",
    "\n",
    "y = x.view(16)               # reshaped the two dimensional array into one dimension\n",
    "print(y)\n",
    "\n",
    "y = x.view(-1, 8)            # -1 lets the pytorch find the first dimension automatically\n",
    "print(y)\n",
    "\n",
    "#y = x.view(8)               # wont run because of size mismatch \n",
    "#print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conversion to other data structures**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xl = [1, 2, 3, 4]               # converting list to tensor\n",
    "xt = torch.tensor(xl)\n",
    "print(xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(5)           # converting tensor into a numpy array\n",
    "print(x)\n",
    "print(type(x))\n",
    "\n",
    "y = x.numpy()\n",
    "print(y)\n",
    "print(type(y))\n",
    "\n",
    "print('----------')\n",
    "\n",
    "x.add_(1)       # we added 1 to each element of x \n",
    "print(x)\n",
    "print(y)        # y also got modified as both share the same memory location (if the tensors are on cpu, not gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.ones(5)                  # converting numpy array into tensor\n",
    "print(a)\n",
    "print(type(a))\n",
    "\n",
    "b = torch.from_numpy(a)\n",
    "print(b)\n",
    "print(type(b))\n",
    "\n",
    "print('----------')\n",
    "\n",
    "a += 1                          # incrementing all elements by 1\n",
    "print(a)\n",
    "print(b)                        # b also got modified, when modifying a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    \n",
    "    device = torch.device(\"cuda\")         # specifying the cuda device\n",
    "    \n",
    "    x = torch.ones(5, device = device)    # creats the tensor on the gpu\n",
    "    \n",
    "    y = torch.ones(5)                     # second option is to first create it\n",
    "    y = y.to(device)                      # and then move it gpu \n",
    "    \n",
    "    z = x + y                             # this operation will be performed on gpu\n",
    "    \n",
    "    z.numpy()           # wont work because numpy can only handle cpu tensor, cant convert gpu tensor \n",
    "    \n",
    "    z = z.to(\"cpu\")     # so to convert the tensor to numpy, we move it to cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='autograd'></a>\n",
    "## 2.  Introduction to Autograd\n",
    "\n",
    "Let's see how to use the autograd package of PyTorch to compute the gradients.\n",
    "<br>We need to compute gradients wrt model parameters, in order to optimize the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.randn(3, requires_grad = True)   # we want to calculate gradient of some function wrt x\n",
    "print(x)               # think of x as model parameter (theta), and the function as the cost function J\n",
    "print(x.grad)          # whenever we do operations with this tensor, pytorch will create a computational graph\n",
    "\n",
    "y = x + 2      # forward propagation: since we specified x with requires_grad = True,\n",
    "print(y)       # Pytorch creates a gradient function, grad_fn = AddBackward (since the step is addition)\n",
    "               # This function is then used in backpropagation to calculate gradients: gradient of y wrt x\n",
    "\n",
    "z = y * y * 2\n",
    "print(z)           # grad_fn is MulBackward (previous step operation is multiplication)\n",
    "\n",
    "z = z.mean()\n",
    "print(z)           # grad_fn in this case is MeanBackward\n",
    "\n",
    "z.backward()       # calculates dz/dx: gradient of z with respect to x\n",
    "print(x.grad)      # Note: no argument is required in backward() since z is a scalar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variables for which requires_grad = True, and the variables which are functions of these variables directly or indirectly define the computation graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note 1: If we do not mention required_grad = True, its default value is False, and we will not have grad_fn attribute in y and z above, and thus we will not be able to call backward function on the output. (Give it a try !)\n",
    "\n",
    "Note 2: In z.backward(), no argument is required only when z is scalar. If z is vector, we need to pass argument. Try calculating z.backward(), without taking the mean step. It will not work as z will be vector in that case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three ways in which we can prevent PyTorch from creating the gradient functions (grad_fn) and tracking the history in the computational graph.\n",
    "<br>For example, during training loop, when we update our weights, then this operation should not be part of gradient computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(3, requires_grad = True)\n",
    "print(x)\n",
    "x.requires_grad_(False)       # option 1: call requires_grad function and set it to False\n",
    "print(x)                      # any function with trailing underscores make the changes in-place\n",
    "print('----------')\n",
    "\n",
    "x = torch.randn(3, requires_grad = True)\n",
    "print(x)\n",
    "y = x.detach()                # option 2: create new tensor by detaching the gradient part\n",
    "print(y)\n",
    "print('----------')\n",
    "\n",
    "with torch.no_grad():         # option 3: wrap under with using no_grad function\n",
    "    y = x + 2\n",
    "    print(x)                  \n",
    "    print(y)                  # even though x has requires_grad = True, y has no attribute grad_fn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, if we dont want a variable which is a function of a variable with requires_grad = True to be part of computational graph, we have to use above methods "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whenever we call the backward function, then the gradient for the tensor will be accumulated into the .grad attribute. \n",
    "<br>Let creat a dummy training example to understand this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.ones(4, requires_grad = True)\n",
    "\n",
    "for epoch in range(3):\n",
    "    \n",
    "    model_output = (weights*3).sum()    # forward propagation\n",
    "    model_output.backward()             # gradient computation\n",
    "    print(weights.grad)\n",
    "    \n",
    "    weights.grad.zero_()                 # we need to use this to stop accumulation                   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='backpropagation'></a>\n",
    "## 3. A simple illustration of Backpropagation using Autograd\n",
    "\n",
    "Let's do one step of forward and backward propagation to understand how it works. \n",
    "<br>We take a dummy model with one example (one input x, one output y), and one weight (one feature).\n",
    "\n",
    "Gradient of loss wrt to weight w: dw = d(Loss)/dw = (d(Loss)/dy_hat) * (dy_hat/dw) = 2(y_hat - y) * x = 2(1 - 2) * 1 = -2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(1.0)       # input feature: value 1\n",
    "y = torch.tensor(2.0)       # target feature: value 2\n",
    "\n",
    "w = torch.tensor(1.0, requires_grad = True)     # weight (initial value 1): \n",
    "                                                # model parameter so we set requires_grad = True\n",
    "\n",
    "y_hat = w * x               # forward pass\n",
    "loss = (y_hat - y)**2\n",
    "print(loss)\n",
    "\n",
    "loss.backward()             # backward pass\n",
    "print(w.grad)               # we have the gradient value after the first pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output w.grad = -2, matching with our exact calculation for this dummy problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='linear_regression_manual'></a>\n",
    "## 4. Illustrative linear regression: A manual implementation\n",
    "\n",
    "Before we learn to develop a machine learning model using PyTorch, lets do it manually first, taking the case of linear regression.\n",
    "<br> Here, we manually construct the steps for prediction, loss calculation and parameter update, using Autograd package only for computing gradients.\n",
    "<br> In the next step, we replace these manual updates using PyTorch utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "X = torch.tensor([1, 2, 3, 4], dtype = torch.float32)   # Our dataset is such that we expect weight w to be 2\n",
    "y = torch.tensor([2, 4, 6, 8], dtype = torch.float32) \n",
    "\n",
    "w = torch.tensor(0.0, dtype = torch.float32, requires_grad = True)\n",
    "\n",
    "def forward(x):                   \n",
    "    return w * x\n",
    "\n",
    "def loss(y, y_pred):\n",
    "    return ((y - y_pred)**2).mean()\n",
    "\n",
    "learning_rate = 0.01\n",
    "n_epochs = 100\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    y_pred = forward(X)             # forward propagation\n",
    "    l = loss(y, y_pred)             # calculate loss\n",
    "    l.backward()                    # backprop to calculate gradient: d(loss)/dw\n",
    "    \n",
    "    with torch.no_grad():            # to prevent this calculation to be part of computational graph\n",
    "        w -= learning_rate * w.grad  # update weight\n",
    "        \n",
    "    w.grad.zero_()                     # zero gradients\n",
    "      \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'epoch: {epoch+1}; loss: {l:.6f}; weight: {w:.6f}')\n",
    "\n",
    "print(f'Prediction after training: f(5) = {forward(5):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='linear_regression_pytorch'></a>\n",
    "## 5. Illustrative linear regression: A Pytorch implementation\n",
    "\n",
    "Now, lets move to the next step, \n",
    " - replacing manually computed loss and parameter updates by using Loss and Optimizer classes.\n",
    " - replacing the manually computed model prediction by implementing a PyTorch model. \n",
    "\n",
    "In general, training a machine learning model has four steps:\n",
    " - forward propagation to compute the prediction: we define a model for this step\n",
    " - loss computation: we define a loss function for this step\n",
    " - backpropagation for gradient computation: we just do loss.backward()\n",
    " - parameters update: we need to define an optimizer, with model parameters and learning rate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn       # we import the neural network to use some functions\n",
    "\n",
    "X = torch.tensor([[1], [2], [3], [4]], dtype = torch.float32)   # we change X, y to be 2d array\n",
    "y = torch.tensor([[2], [4], [6], [8]], dtype = torch.float32)\n",
    "n_samples, n_features = X.shape\n",
    "\n",
    "# we do not need to define the weight w explicitly, as our PyTorch model knows the parameters\n",
    "\n",
    "# model designing\n",
    "input_size = n_features\n",
    "output_size = n_features\n",
    "model = nn.Linear(input_size, output_size)  # we define layer here: we have only one layer\n",
    "                                            # we use the built-in layer, Linear\n",
    "    \n",
    "# loss and optimizer\n",
    "learning_rate = 0.01\n",
    "loss = nn.MSELoss()                 # replacing the manually defined loss, MSE: Mean Squared Error\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr =  learning_rate)   # defining an optimizer\n",
    "\n",
    "# training loop\n",
    "n_epochs = 1000\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    y_pred = model(X)           # forward propagation\n",
    "    l = loss(y, y_pred)         # calculate loss\n",
    "    l.backward()                # backprop to calculate gradient: d(loss)/dw   \n",
    "    optimizer.step()            # replacing the manual update weight\n",
    "    \n",
    "    optimizer.zero_grad()       # setting the gradient zero\n",
    "\n",
    "    if (epoch+1) % 50 == 0:\n",
    "        [w, b] = model.parameters()       # unpack the parameters, w is list of lists\n",
    "        print(f'epoch: {epoch+1}; loss: {l:.6f}; weight: {w[0][0]:.6f}')\n",
    "\n",
    "X_test = torch.tensor([5], dtype = torch.float32)\n",
    "print(f'Prediction after training: f(5) = {model(X_test).item():.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Custom Model**\n",
    "\n",
    "In the above implementation, we did not have to come up with a model for ourselves. We needed only one layer, and it was provided by PyTorch. \n",
    "<br> In most cases, we will need to build a custom model. Let's implement the code again by building a custom model.\n",
    "\n",
    "Let's first define the class for custom model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn   \n",
    "\n",
    "class LinearRegression(nn.Module):                      # deriving from nn.Module\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        \n",
    "        super(LinearRegression, self).__init__()\n",
    "        self.lin = nn.Linear(input_dim, output_dim)     # define our layers\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        return self.lin(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we define our model by using this custom defined class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor([[1], [2], [3], [4]], dtype = torch.float32)\n",
    "y = torch.tensor([[2], [4], [6], [8]], dtype = torch.float32)\n",
    "n_samples, n_features = X.shape\n",
    "\n",
    "# model designing\n",
    "input_size = n_features\n",
    "output_size = n_features\n",
    "model = LinearRegression(input_size, output_size)    # defining the model \n",
    "\n",
    "# loss and optimizer\n",
    "learning_rate = 0.01\n",
    "loss = nn.MSELoss()             \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr =  learning_rate)   \n",
    "\n",
    "# training loop\n",
    "n_epochs = 1000\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    y_pred = model(X)           # forward propagation\n",
    "    l = loss(y, y_pred)         # calculate loss\n",
    "    l.backward()                # backprop to calculate gradient: d(loss)/dw   \n",
    "    optimizer.step()            # replacing the manual update weight\n",
    "    \n",
    "    optimizer.zero_grad()       # setting the gradient zero\n",
    "\n",
    "    if (epoch+1) % 50 == 0:\n",
    "        [w, b] = model.parameters()\n",
    "        print(f'epoch: {epoch+1}; loss: {l:.6f}; weight: {w[0][0]:.6f}')\n",
    "\n",
    "X_test = torch.tensor([5], dtype = torch.float32)\n",
    "print(f'Prediction after training: f(5) = {model(X_test).item():.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='linear_regression'></a>\n",
    "## 6. Linear Regression\n",
    "\n",
    "Now, lets build a regression model using datasets provided by sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# generating regression data\n",
    "X_numpy, y_numpy = datasets.make_regression(n_samples = 100, \n",
    "                                            n_features = 1, \n",
    "                                            noise = 20, \n",
    "                                            random_state = 1)\n",
    "\n",
    "X = torch.from_numpy(X_numpy.astype(np.float32))  # converting numpy array into tensors\n",
    "y = torch.from_numpy(y_numpy.astype(np.float32))\n",
    "y = y.view(y.shape[0], 1)                         # convert y into one column (it is one row)\n",
    "\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "\n",
    "# model designing\n",
    "input_size = n_features\n",
    "output_size = 1\n",
    "model = nn.Linear(input_size, output_size)\n",
    "\n",
    "# loss and optimizer\n",
    "learning_rate = 0.01\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
    "\n",
    "# training loop\n",
    "n_epochs = 100\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    y_pred = model(X)\n",
    "    loss = criterion(y_pred, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'epoch = {epoch+1}; loss = {loss.item():.4f}')\n",
    "        \n",
    "        \n",
    "predicted = model(X).detach().numpy()\n",
    "plt.plot(X_numpy, y_numpy, 'ro')\n",
    "plt.plot(X_numpy, predicted, 'b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='logistic_regression'></a>\n",
    "## 7. Logistic Regression\n",
    "\n",
    "Let's build a logistic regression model to classify breast tumor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "bc = datasets.load_breast_cancer()        # generating logistic regression data\n",
    "X, y = bc.data, bc.target\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "print(n_samples, n_features)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1234)\n",
    "\n",
    "sc = StandardScaler()                 # to scale our features having zero mean and unit variance\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "X_train = torch.from_numpy(X_train.astype(np.float32))\n",
    "X_test = torch.from_numpy(X_test.astype(np.float32))\n",
    "y_train = torch.from_numpy(y_train.astype(np.float32))\n",
    "y_test = torch.from_numpy(y_test.astype(np.float32))\n",
    "\n",
    "print('X shape: ', X_train.shape, X_test.shape)\n",
    "\n",
    "print('y shape: ', y_train.shape, y_test.shape)\n",
    "\n",
    "y_train = y_train.view(y_train.shape[0], 1)     # making the tensor two dimensional\n",
    "y_test = y_test.view(y_test.shape[0], 1)\n",
    "\n",
    "print('y shape: ', y_train.shape, y_test.shape)\n",
    "\n",
    "# model designing: here we write our own class \n",
    "class LogisticRegression(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_input_features):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(n_input_features, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y_predicted = torch.sigmoid(self.linear(x))\n",
    "        return y_predicted\n",
    "\n",
    "model = LogisticRegression(n_features)\n",
    "\n",
    "# loss and optimizer\n",
    "learning_rate = 0.01\n",
    "criterion = nn.BCELoss()            # BCE: Binary Cross Entropy loss\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
    "\n",
    "# training loop\n",
    "n_epochs = 100\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    y_pred = model(X_train)\n",
    "    loss = criterion(y_pred, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'epoch: {epoch+1}; loss = {loss.item():.4f}')\n",
    "        \n",
    "with torch.no_grad():                   # to avoid this part of the computational graph\n",
    "    y_pred = model(X_test)\n",
    "    y_pred_class = y_pred.round()\n",
    "    accuracy = y_pred_class.eq(y_test).sum()/float(y_test.shape[0])\n",
    "    print(f'accuracy = {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='dataset_dataloader'></a>\n",
    "## 8. Batch Training: Dataset and DataLoader Classes\n",
    "\n",
    "If we use our whole training data at each step for optimizing our model, the computation will be time consuming. \n",
    "<br>So, a better way to deal with large datasets is to divide the whole sample into small batches and optimize our model on batches one at a time. \n",
    "<br>For this purpose, we have Dataset and DataLoader classes in PyTorch.\n",
    "\n",
    "Let's define some terms for clarity:\n",
    " - epoch: one forward and backward pass of all training samples\n",
    " - batch_size: number of training samples in one forward and backward pass\n",
    " - number of iterations: number of passes, each pass using [batch size] number of samples\n",
    " - For example, with 100 training samples, and batch_size = 20, we have 100/20 = 5 iterations for one epoch\n",
    " \n",
    "Info about the dataset wine.csv:\n",
    " - first column is class labels: three categories of wine labeled 1, 2, 3\n",
    " - rest of the columns are features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# implementing our custom dataset\n",
    "class WineDataset(Dataset):              # inherits Dataset\n",
    "    \n",
    "    def __init__(self):                  # load dataset, split into x and y\n",
    "        xy = np.loadtxt('wine.csv', delimiter = \",\", dtype = np.float32, skiprows = 1)\n",
    "        self.x = torch.from_numpy(xy[:, 1:])  # all the rows, all the columns except first one\n",
    "        self.y = torch.from_numpy(xy[:, [0]]) # all the rows, first column only\n",
    "        self.n_samples = xy.shape[0]\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "dataset = WineDataset()\n",
    "print('feature/target shape:', dataset.x.size(), dataset.y.size())\n",
    "print('----------')\n",
    "\n",
    "# verifying the WineDataset class\n",
    "first_data = dataset[0]              # get the first row\n",
    "print(first_data)                    # tuple of two tensors: feature and target\n",
    "features, labels = first_data        # unpack\n",
    "print(features, labels)\n",
    "print('----------')\n",
    "\n",
    "batch_size = 4\n",
    "dataloader = DataLoader(dataset = dataset, \n",
    "                        batch_size = batch_size, \n",
    "                        shuffle = True, \n",
    "                        num_workers = 2)\n",
    "datatiter = iter(dataloader)                 # converting the object to an interator\n",
    "data = datatiter.next()                      # obtaining the next batch of data     \n",
    "features, labels = data                      # unpack\n",
    "print(features, labels)                      # four examples, as our batch_size is 4\n",
    "\n",
    "# let's create a dummy training loop to iterate over the whole dataloader\n",
    "n_epochs = 2\n",
    "total_samples = len(dataset)\n",
    "n_iterations = math.ceil(total_samples/batch_size)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i, (inputs, labels) in enumerate(dataloader):\n",
    "        if (i+1) % 5 == 0:\n",
    "            print(f'epoch: {epoch+1}/{n_epochs}, step: {i+1}/{n_iterations}, inputs: {inputs.shape} ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='dataset_transforms'></a>\n",
    "## 9. Dataset Transforms\n",
    "\n",
    "\n",
    "Transforms help us in doing tranformation on datasets. PyTorch has a lot of inbuilt transform classes which we can use. Additionally, we can define our own custom transformations.\n",
    "\n",
    "In the previous section, we implemented a custom WineDataset. Let's extend this class to support data transformation, and define custom tranforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "class WineDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, transform = None):\n",
    "        xy = np.loadtxt('wine.csv', delimiter = ',', dtype = np.float32, skiprows = 1)\n",
    "        self.x = xy[:, 1:]       # we do not convert x, y to tensors\n",
    "        self.y = xy[:, [0]]\n",
    "        self.n_samples = xy.shape[0]\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        sample = self.x[index], self.y[index]\n",
    "        if self.transform:                         # apply transformation if available\n",
    "            sample = self.transform(sample)\n",
    "        return sample\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "    \n",
    "# defining a custom transform class \n",
    "class ToTensor:                                    # class to transform to tensors\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        inputs, targets = sample\n",
    "        return torch.from_numpy(inputs), torch.from_numpy(targets)\n",
    "    \n",
    "# defining another custom tranform class\n",
    "class MulTransform:                                # multiplication transform\n",
    "    \n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "        \n",
    "    def __call__(self, sample):\n",
    "        inputs, target = sample\n",
    "        inputs *= self.factor\n",
    "        return inputs, target\n",
    "    \n",
    "dataset = WineDataset()                            # not calling the ToTensor transform\n",
    "first_data = dataset[0]\n",
    "features, labels = first_data\n",
    "print(type(features), type(labels))                # class numpy.ndarray\n",
    "print(features)\n",
    "print('----------')\n",
    "\n",
    "dataset = WineDataset(transform = ToTensor())      # calling the ToTensor transform \n",
    "first_data = dataset[0] \n",
    "features, labels = first_data\n",
    "print(type(features), type(labels))                # class: torch.Tensor\n",
    "print(features)\n",
    "print('----------')\n",
    "\n",
    "composed = torchvision.transforms.Compose([ToTensor(), MulTransform(2)])  # defining a composed transform\n",
    "dataset = WineDataset(transform = composed)\n",
    "first_data = dataset[0] \n",
    "features, labels = first_data\n",
    "print(type(features), type(labels))\n",
    "print(features)                           # each feature value got doubled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='softmax_cross_entropy'></a>\n",
    "## 10. Softmax and Cross-Entropy \n",
    "\n",
    "Now, let's discuss softmax function and the cross-entropy loss, most commonly used functions in neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Softmax**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    return np.exp(x)/np.sum(np.exp(x), axis = 0)\n",
    "\n",
    "x = np.array([2.0, 1.0, 0.1])             # using numpy\n",
    "outputs = softmax(x)\n",
    "print(outputs)\n",
    "print('----------')\n",
    "\n",
    "x = torch.tensor([2.0, 1.0, 0.1])         # using torch             \n",
    "outputs = torch.softmax(x, dim = 0)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cross-Entropy Loss**\n",
    "\n",
    "Now, let's discuss cross-entropy loss, which measures the performance in multi-class classification problems. \n",
    "The loss increases as the predicted probability diverges from the actual label.\n",
    "\n",
    "Let's assume we have one sample, and three classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "def cross_entropy(predicted, actual):\n",
    "    \n",
    "    loss = -np.sum(actual * np.log(predicted))\n",
    "    return loss\n",
    "\n",
    "y = np.array([1, 0, 0])                             # must be one-hot encoded: three classes here\n",
    "y_pred_good = np.array([0.6590, 0.2424, 0.0986])    # numbers after applying the softmax\n",
    "y_pred_bad = np.array([0.1, 0.3, 0.6])\n",
    "loss_good = cross_entropy(y_pred_good, y)\n",
    "loss_bad = cross_entropy(y_pred_bad, y)\n",
    "print('Numpy: ', loss_good, loss_bad)\n",
    "\n",
    "# In PyTorch implementation, there are two differences\n",
    "# first, we provide the correct class label, not the one-hot encoded\n",
    "# second, softmax is not needed, it is already implemented\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "y = torch.tensor([0])                # correct class label which is 0, not the one-hot encoded\n",
    "y_pred_good = torch.tensor([[2.0, 1.0, 0.1]])   # CrossEntropyLoss already has inbuilt softmax\n",
    "y_pred_bad = torch.tensor([[0.5, 2.0, 0.3]])    # we pass the output without applying the softmax\n",
    "loss_good = loss(y_pred_good, y)    # y_pred dimension should be n_samples * n_classes\n",
    "loss_bad = loss(y_pred_bad, y)\n",
    "print('PyTorch:', loss_good.item(), loss_bad.item())\n",
    "\n",
    "# to get the actual predictions\n",
    "_, prediction1 = torch.max(y_pred_good, 1)         # 1: along the first dimensions\n",
    "_, prediction2 = torch.max(y_pred_bad, 1)\n",
    "print(prediction1, prediction2)                    # chooses the class with highest probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss in PyTorch allows for multiple samples. Lets assume we have 3 samples and 3 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "y = torch.tensor([2, 0, 1])                   # the actual labels for the three samples\n",
    "y_pred = torch.tensor([[0.1, 1.0, 2.0], \n",
    "                       [2.0, 1.0, 0.1], \n",
    "                       [0.1, 2.0, 1.0]])      # highest value for the correct labels\n",
    "\n",
    "closs = loss(y_pred, y)\n",
    "_, predictions = torch.max(y_pred, 1)\n",
    "print(closs, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** In multi-class classification problem, we use nn.CrossEntropyLoss() at the end of the neural network to compute the loss. Since nn.CrossEntropyLoss() automatically implements the softmax, we do not implement the softmax layer before the loss calculation, to convert the numbers into probabilities.\n",
    "\n",
    "However, in case of binary classification, we use nn.BCELoss() to calculate the loss. In binary classification, we have only one node in the end, and we apply sigmoid function on the output of this node, and then feed it to nn.BCELoss(). The loss function nn.BCELoss() does not automatically implement the sigmoid function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='activation_functions'></a>\n",
    "## 11. Activation Functions\n",
    "\n",
    "Some of the most popular activation functions are:\n",
    " - Sigmoid: Typically used in the last layer of a binary classification problem\n",
    " - Tanh: A good choice for hidden layers\n",
    " - ReLU: Most popular choice for hidden layers\n",
    " - Leaky ReLU: Improved version of ReLU; tries to solve the vanishing gradient problem\n",
    " - Softmax: Good in last layer in multiclass classification problem\n",
    " \n",
    "Let's write an illustrative code showing how to use these activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class NeuralNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size):               # we define all the layers\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)      # first linear layer\n",
    "        self.relu = nn.ReLU()                                  # ReLU activation function\n",
    "        self.linear2 = nn.Linear(hidden_size, 1)               # next linear layer\n",
    "        self.sigmoid = nn.Sigmoid()                            # Sigmoid: next activation function \n",
    "        \n",
    "    # In the forward pass, we call the above functions after each other\n",
    "    def forward(self, x):\n",
    "        out = self.linear1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to write the above code is by using the activation functions directly in forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class NeuralNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size):               # we define all the layers\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)      # first linear layer\n",
    "        self.linear2 = nn.Linear(hidden_size, 1)               # next linear layer\n",
    "        \n",
    "    # In the forward pass, we call the above functions after each other\n",
    "    def forward(self, x):\n",
    "        out = torch.relu(self.linear1(x))\n",
    "        out = torch.sigmoid(self.linear2(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both of these codes work same, its just a matter of taste which way one writes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
