{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image classification: Convolutional Neural Network\n",
    "\n",
    "In this notebook, we implement a convolutional neural network for image classification. We use the very popular CIFAR-10 dataset, which has images belonging to ten classes: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "dataset has PILImage images of range [0, 1], which we transform to tensors of normalized range (-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), \n",
    "                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(root = './data',\n",
    "                                            train = True,\n",
    "                                            download = True,\n",
    "                                            transform = transform)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root = './data',\n",
    "                                           train = False,\n",
    "                                           transform = transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset CIFAR10\n",
      "    Number of datapoints: 50000\n",
      "    Root location: ./data\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "               Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
      "           )\n",
      "--------------------\n",
      "Dataset CIFAR10\n",
      "    Number of datapoints: 10000\n",
      "    Root location: ./data\n",
      "    Split: Test\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "               Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset)\n",
    "print('--------------------')\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "torch.Size([3, 32, 32]) 6\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset))\n",
    "\n",
    "image, label = train_dataset[0]\n",
    "print(image.size(), label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                          batch_size = batch_size,\n",
    "                                          shuffle = True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                         batch_size = batch_size,\n",
    "                                         shuffle = False)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', \n",
    "           'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and Training\n",
    "\n",
    "The size of an image after employing convolution is (W-F+2P)/S + 1, where W is input width, F is filter size and P is padding and S is stride.\n",
    "<br>Thus, if we apply convolution on an image of size 5 * 5 (W = 5), by 3 * 3 filter (F = 3), given no padding (P = 0) and stride (S = 1), the width of the output convoluted image is (5 - 3 + 0)/1 + 1 = 3, i.e, we have 3 * 3 image.\n",
    "\n",
    "\n",
    "Let's first define the ConvNet model we want to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, ):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)   # input channel size = 3, output channel size = 6, kernel size = 5 (5 by 5) \n",
    "        self.pool = nn.MaxPool2d(2, 2)    # kernel size = 2 (2 by 2 kernel), stride = 2\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)  # input channel size must be equal to last output channel size\n",
    "        self.fc1 = nn.Linear(16*5*5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "           \n",
    "    def forward(self, x):\n",
    "        out = torch.relu(self.conv1(x))      # first convolutional layer\n",
    "        out = self.pool(out)                 # pooling layer\n",
    "        out = torch.relu(self.conv2(out))    # second convolutional layer\n",
    "        out = self.pool(out)                 # pooling layer\n",
    "        out = out.view(-1, 16*5*5)           # flattening the tensor\n",
    "        out = torch.relu(self.fc1(out))      # first fully connected layer\n",
    "        out = torch.relu(self.fc2(out))      # second fully connected layer\n",
    "        out = self.fc3(out)                  # third fully connected layer\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "epoch: 1/20; step: 1000/10000; loss: 2.3043\n",
      "epoch: 1/20; step: 2000/10000; loss: 2.2993\n",
      "epoch: 1/20; step: 3000/10000; loss: 2.3186\n",
      "epoch: 1/20; step: 4000/10000; loss: 2.3009\n",
      "epoch: 1/20; step: 5000/10000; loss: 2.3042\n",
      "epoch: 1/20; step: 6000/10000; loss: 2.3120\n",
      "epoch: 1/20; step: 7000/10000; loss: 2.2874\n",
      "epoch: 1/20; step: 8000/10000; loss: 2.2889\n",
      "epoch: 1/20; step: 9000/10000; loss: 2.3277\n",
      "epoch: 1/20; step: 10000/10000; loss: 2.2163\n",
      "epoch: 2/20; step: 1000/10000; loss: 2.2970\n",
      "epoch: 2/20; step: 2000/10000; loss: 2.1748\n",
      "epoch: 2/20; step: 3000/10000; loss: 2.3356\n",
      "epoch: 2/20; step: 4000/10000; loss: 2.1196\n",
      "epoch: 2/20; step: 5000/10000; loss: 2.3641\n",
      "epoch: 2/20; step: 6000/10000; loss: 1.8763\n",
      "epoch: 2/20; step: 7000/10000; loss: 1.6302\n",
      "epoch: 2/20; step: 8000/10000; loss: 2.0208\n",
      "epoch: 2/20; step: 9000/10000; loss: 1.9638\n",
      "epoch: 2/20; step: 10000/10000; loss: 1.7376\n",
      "epoch: 3/20; step: 1000/10000; loss: 1.7287\n",
      "epoch: 3/20; step: 2000/10000; loss: 1.3165\n",
      "epoch: 3/20; step: 3000/10000; loss: 1.6525\n",
      "epoch: 3/20; step: 4000/10000; loss: 1.7594\n",
      "epoch: 3/20; step: 5000/10000; loss: 1.9965\n",
      "epoch: 3/20; step: 6000/10000; loss: 2.0818\n",
      "epoch: 3/20; step: 7000/10000; loss: 1.8042\n",
      "epoch: 3/20; step: 8000/10000; loss: 1.8511\n",
      "epoch: 3/20; step: 9000/10000; loss: 2.1253\n",
      "epoch: 3/20; step: 10000/10000; loss: 1.8358\n",
      "epoch: 4/20; step: 1000/10000; loss: 1.3972\n",
      "epoch: 4/20; step: 2000/10000; loss: 1.5617\n",
      "epoch: 4/20; step: 3000/10000; loss: 1.3299\n",
      "epoch: 4/20; step: 4000/10000; loss: 1.3742\n",
      "epoch: 4/20; step: 5000/10000; loss: 1.3192\n",
      "epoch: 4/20; step: 6000/10000; loss: 1.6728\n",
      "epoch: 4/20; step: 7000/10000; loss: 0.9904\n",
      "epoch: 4/20; step: 8000/10000; loss: 2.1577\n",
      "epoch: 4/20; step: 9000/10000; loss: 1.3845\n",
      "epoch: 4/20; step: 10000/10000; loss: 1.2189\n",
      "epoch: 5/20; step: 1000/10000; loss: 0.9479\n",
      "epoch: 5/20; step: 2000/10000; loss: 0.9683\n",
      "epoch: 5/20; step: 3000/10000; loss: 1.3433\n",
      "epoch: 5/20; step: 4000/10000; loss: 1.8310\n",
      "epoch: 5/20; step: 5000/10000; loss: 1.6017\n",
      "epoch: 5/20; step: 6000/10000; loss: 1.5055\n",
      "epoch: 5/20; step: 7000/10000; loss: 1.7571\n",
      "epoch: 5/20; step: 8000/10000; loss: 1.6646\n",
      "epoch: 5/20; step: 9000/10000; loss: 1.6120\n",
      "epoch: 5/20; step: 10000/10000; loss: 1.3352\n",
      "epoch: 6/20; step: 1000/10000; loss: 1.7429\n",
      "epoch: 6/20; step: 2000/10000; loss: 2.2530\n",
      "epoch: 6/20; step: 3000/10000; loss: 0.6848\n",
      "epoch: 6/20; step: 4000/10000; loss: 1.0831\n",
      "epoch: 6/20; step: 5000/10000; loss: 0.6658\n",
      "epoch: 6/20; step: 6000/10000; loss: 1.2718\n",
      "epoch: 6/20; step: 7000/10000; loss: 1.5124\n",
      "epoch: 6/20; step: 8000/10000; loss: 1.0647\n",
      "epoch: 6/20; step: 9000/10000; loss: 1.2277\n",
      "epoch: 6/20; step: 10000/10000; loss: 2.7106\n",
      "epoch: 7/20; step: 1000/10000; loss: 1.7031\n",
      "epoch: 7/20; step: 2000/10000; loss: 1.2745\n",
      "epoch: 7/20; step: 3000/10000; loss: 1.3317\n",
      "epoch: 7/20; step: 4000/10000; loss: 0.9651\n",
      "epoch: 7/20; step: 5000/10000; loss: 2.0094\n",
      "epoch: 7/20; step: 6000/10000; loss: 1.2076\n",
      "epoch: 7/20; step: 7000/10000; loss: 1.6221\n",
      "epoch: 7/20; step: 8000/10000; loss: 1.6037\n",
      "epoch: 7/20; step: 9000/10000; loss: 1.9756\n",
      "epoch: 7/20; step: 10000/10000; loss: 1.1799\n",
      "epoch: 8/20; step: 1000/10000; loss: 1.6591\n",
      "epoch: 8/20; step: 2000/10000; loss: 0.3972\n",
      "epoch: 8/20; step: 3000/10000; loss: 2.2796\n",
      "epoch: 8/20; step: 4000/10000; loss: 1.6262\n",
      "epoch: 8/20; step: 5000/10000; loss: 1.3136\n",
      "epoch: 8/20; step: 6000/10000; loss: 1.4883\n",
      "epoch: 8/20; step: 7000/10000; loss: 1.3638\n",
      "epoch: 8/20; step: 8000/10000; loss: 1.0421\n",
      "epoch: 8/20; step: 9000/10000; loss: 2.1131\n",
      "epoch: 8/20; step: 10000/10000; loss: 1.4220\n",
      "epoch: 9/20; step: 1000/10000; loss: 0.8595\n",
      "epoch: 9/20; step: 2000/10000; loss: 0.9825\n",
      "epoch: 9/20; step: 3000/10000; loss: 1.3286\n",
      "epoch: 9/20; step: 4000/10000; loss: 1.7268\n",
      "epoch: 9/20; step: 5000/10000; loss: 0.8874\n",
      "epoch: 9/20; step: 6000/10000; loss: 1.1873\n",
      "epoch: 9/20; step: 7000/10000; loss: 1.6105\n",
      "epoch: 9/20; step: 8000/10000; loss: 1.6692\n",
      "epoch: 9/20; step: 9000/10000; loss: 1.1496\n",
      "epoch: 9/20; step: 10000/10000; loss: 1.0997\n",
      "epoch: 10/20; step: 1000/10000; loss: 0.9760\n",
      "epoch: 10/20; step: 2000/10000; loss: 0.9876\n",
      "epoch: 10/20; step: 3000/10000; loss: 1.5051\n",
      "epoch: 10/20; step: 4000/10000; loss: 0.7423\n",
      "epoch: 10/20; step: 5000/10000; loss: 1.1237\n",
      "epoch: 10/20; step: 6000/10000; loss: 1.3097\n",
      "epoch: 10/20; step: 7000/10000; loss: 1.1787\n",
      "epoch: 10/20; step: 8000/10000; loss: 1.0744\n",
      "epoch: 10/20; step: 9000/10000; loss: 1.1905\n",
      "epoch: 10/20; step: 10000/10000; loss: 1.0151\n",
      "epoch: 11/20; step: 1000/10000; loss: 1.1669\n",
      "epoch: 11/20; step: 2000/10000; loss: 0.9633\n",
      "epoch: 11/20; step: 3000/10000; loss: 1.0194\n",
      "epoch: 11/20; step: 4000/10000; loss: 1.0561\n",
      "epoch: 11/20; step: 5000/10000; loss: 0.5454\n",
      "epoch: 11/20; step: 6000/10000; loss: 0.5023\n",
      "epoch: 11/20; step: 7000/10000; loss: 0.6542\n",
      "epoch: 11/20; step: 8000/10000; loss: 0.5296\n",
      "epoch: 11/20; step: 9000/10000; loss: 1.3414\n",
      "epoch: 11/20; step: 10000/10000; loss: 1.7347\n",
      "epoch: 12/20; step: 1000/10000; loss: 1.3880\n",
      "epoch: 12/20; step: 2000/10000; loss: 0.8965\n",
      "epoch: 12/20; step: 3000/10000; loss: 0.9997\n",
      "epoch: 12/20; step: 4000/10000; loss: 1.2438\n",
      "epoch: 12/20; step: 5000/10000; loss: 0.5853\n",
      "epoch: 12/20; step: 6000/10000; loss: 1.4304\n",
      "epoch: 12/20; step: 7000/10000; loss: 1.2482\n",
      "epoch: 12/20; step: 8000/10000; loss: 0.7289\n",
      "epoch: 12/20; step: 9000/10000; loss: 0.9124\n",
      "epoch: 12/20; step: 10000/10000; loss: 1.2792\n",
      "epoch: 13/20; step: 1000/10000; loss: 0.9359\n",
      "epoch: 13/20; step: 2000/10000; loss: 1.6004\n",
      "epoch: 13/20; step: 3000/10000; loss: 1.2884\n",
      "epoch: 13/20; step: 4000/10000; loss: 1.6634\n",
      "epoch: 13/20; step: 5000/10000; loss: 1.8186\n",
      "epoch: 13/20; step: 6000/10000; loss: 0.7046\n",
      "epoch: 13/20; step: 7000/10000; loss: 1.3357\n",
      "epoch: 13/20; step: 8000/10000; loss: 1.2446\n",
      "epoch: 13/20; step: 9000/10000; loss: 1.2240\n",
      "epoch: 13/20; step: 10000/10000; loss: 0.6369\n",
      "epoch: 14/20; step: 1000/10000; loss: 1.2576\n",
      "epoch: 14/20; step: 2000/10000; loss: 1.4942\n",
      "epoch: 14/20; step: 3000/10000; loss: 0.7674\n",
      "epoch: 14/20; step: 4000/10000; loss: 1.4092\n",
      "epoch: 14/20; step: 5000/10000; loss: 0.8235\n",
      "epoch: 14/20; step: 6000/10000; loss: 1.6367\n",
      "epoch: 14/20; step: 7000/10000; loss: 2.2152\n",
      "epoch: 14/20; step: 8000/10000; loss: 1.8153\n",
      "epoch: 14/20; step: 9000/10000; loss: 0.6462\n",
      "epoch: 14/20; step: 10000/10000; loss: 0.4922\n",
      "epoch: 15/20; step: 1000/10000; loss: 0.7726\n",
      "epoch: 15/20; step: 2000/10000; loss: 1.4317\n",
      "epoch: 15/20; step: 3000/10000; loss: 1.7139\n",
      "epoch: 15/20; step: 4000/10000; loss: 1.6326\n",
      "epoch: 15/20; step: 5000/10000; loss: 0.9219\n",
      "epoch: 15/20; step: 6000/10000; loss: 0.3844\n",
      "epoch: 15/20; step: 7000/10000; loss: 1.4301\n",
      "epoch: 15/20; step: 8000/10000; loss: 0.7789\n",
      "epoch: 15/20; step: 9000/10000; loss: 1.5050\n",
      "epoch: 15/20; step: 10000/10000; loss: 0.8539\n",
      "epoch: 16/20; step: 1000/10000; loss: 1.0022\n",
      "epoch: 16/20; step: 2000/10000; loss: 1.9129\n",
      "epoch: 16/20; step: 3000/10000; loss: 1.0735\n",
      "epoch: 16/20; step: 4000/10000; loss: 0.8505\n",
      "epoch: 16/20; step: 5000/10000; loss: 0.9584\n",
      "epoch: 16/20; step: 6000/10000; loss: 0.6882\n",
      "epoch: 16/20; step: 7000/10000; loss: 0.8479\n",
      "epoch: 16/20; step: 8000/10000; loss: 0.9392\n",
      "epoch: 16/20; step: 9000/10000; loss: 0.4807\n",
      "epoch: 16/20; step: 10000/10000; loss: 1.0670\n",
      "epoch: 17/20; step: 1000/10000; loss: 0.6667\n",
      "epoch: 17/20; step: 2000/10000; loss: 0.5523\n",
      "epoch: 17/20; step: 3000/10000; loss: 1.2259\n",
      "epoch: 17/20; step: 4000/10000; loss: 1.3966\n",
      "epoch: 17/20; step: 5000/10000; loss: 1.8095\n",
      "epoch: 17/20; step: 6000/10000; loss: 0.7420\n",
      "epoch: 17/20; step: 7000/10000; loss: 0.8000\n",
      "epoch: 17/20; step: 8000/10000; loss: 0.7257\n",
      "epoch: 17/20; step: 9000/10000; loss: 1.8066\n",
      "epoch: 17/20; step: 10000/10000; loss: 1.3214\n",
      "epoch: 18/20; step: 1000/10000; loss: 0.9772\n",
      "epoch: 18/20; step: 2000/10000; loss: 1.3132\n",
      "epoch: 18/20; step: 3000/10000; loss: 1.0016\n",
      "epoch: 18/20; step: 4000/10000; loss: 1.1086\n",
      "epoch: 18/20; step: 5000/10000; loss: 0.6278\n",
      "epoch: 18/20; step: 6000/10000; loss: 0.7413\n",
      "epoch: 18/20; step: 7000/10000; loss: 0.5393\n",
      "epoch: 18/20; step: 8000/10000; loss: 0.9876\n",
      "epoch: 18/20; step: 9000/10000; loss: 0.8116\n",
      "epoch: 18/20; step: 10000/10000; loss: 0.5123\n",
      "epoch: 19/20; step: 1000/10000; loss: 1.0410\n",
      "epoch: 19/20; step: 2000/10000; loss: 0.5746\n",
      "epoch: 19/20; step: 3000/10000; loss: 1.2273\n",
      "epoch: 19/20; step: 4000/10000; loss: 1.3412\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 19/20; step: 5000/10000; loss: 0.6810\n",
      "epoch: 19/20; step: 6000/10000; loss: 0.4845\n",
      "epoch: 19/20; step: 7000/10000; loss: 0.5913\n",
      "epoch: 19/20; step: 8000/10000; loss: 1.3311\n",
      "epoch: 19/20; step: 9000/10000; loss: 0.5690\n",
      "epoch: 19/20; step: 10000/10000; loss: 0.9449\n",
      "epoch: 20/20; step: 1000/10000; loss: 0.9597\n",
      "epoch: 20/20; step: 2000/10000; loss: 1.4032\n",
      "epoch: 20/20; step: 3000/10000; loss: 1.2400\n",
      "epoch: 20/20; step: 4000/10000; loss: 0.2377\n",
      "epoch: 20/20; step: 5000/10000; loss: 0.5256\n",
      "epoch: 20/20; step: 6000/10000; loss: 2.0739\n",
      "epoch: 20/20; step: 7000/10000; loss: 1.7049\n",
      "epoch: 20/20; step: 8000/10000; loss: 1.3427\n",
      "epoch: 20/20; step: 9000/10000; loss: 1.2717\n",
      "epoch: 20/20; step: 10000/10000; loss: 0.7615\n",
      "Overall accuracy 63.1000\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "\n",
    "model = ConvNet().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
    "\n",
    "n_epochs = 20\n",
    "total_batches = len(train_loader)\n",
    "print(total_batches)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        \n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if (i+1) % 1000 == 0:\n",
    "            print(f'epoch: {epoch+1}/{n_epochs}; step: {i+1}/{total_batches}; loss: {loss.item():.4f}')\n",
    "        \n",
    "        \n",
    "with torch.no_grad():\n",
    "    \n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    n_class_correct = [0 for i in range(10)]\n",
    "    n_class_samples = [0 for i in range(10)]\n",
    "    \n",
    "    for images, labels in test_loader:\n",
    "        \n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        n_correct += (predictions == labels).sum().item()\n",
    "        n_samples += labels.shape[0]\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            \n",
    "            label = labels[i]\n",
    "            pred = predictions[i]\n",
    "            \n",
    "            if label == pred:       \n",
    "                n_class_correct[label] += 1\n",
    "            \n",
    "            n_class_samples[label] += 1\n",
    "    \n",
    "    overall_accuracy = 100.0 * n_correct / n_samples\n",
    "    print('Overall accuracy %.4f' % (overall_accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
